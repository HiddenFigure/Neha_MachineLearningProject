{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Unleashing the mystery of Kaggle\n",
    "- How does it all work?\n",
    "- Feature Engineering\n",
    "- Parameter Tuning\n",
    "- Ensembling & Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it all work - Should I trust the public leaderboard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each Kaggle competition has public and private leaderboard. Public leaderboard only uses part of the test dataset to determine the score and the private leaderboard will evaluated using the other part at the end of the competition.\n",
    "- You can find how Kaggle calculate the public and private leaderboard [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/leaderboard).\n",
    "- If the competition has a large training set and a relatively small public test set compared to private test set, you can easily overfit the public test set. In this case, you **should not** trust the public leaderboard. \n",
    "- If the traing set and test set are collected from different time frames, you **must** trust the public leaderboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://s3.amazonaws.com/nycdsabt01/s2-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV or LB?\n",
    "- **TRUST YOUR CV!**\n",
    "- Typical question on smaller datasets: \n",
    " - “I’m doing proper cross-validation and see improvements on my CV score, but public leaderboard is so random and does not correlate at all!”\n",
    "- Top kagglers’ pick most of the time:\n",
    " - Final Submission = $X*CV + (1-X)*LB$, typically $X=0.5$ is OK.\n",
    "- Trusting CV is a hard thing to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dataframes before we dive into the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 'Id' column\n",
    "train_ID = train_df['Id']\n",
    "test_ID = test_df['Id']\n",
    "\n",
    "# Now drop the 'Id' colum since we can not use it as a feature to train our model.\n",
    "train_df.drop(\"Id\", axis = 1, inplace = True)\n",
    "test_df.drop(\"Id\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train_df['SalePrice']\n",
    "X_train = train_df.drop('SalePrice', axis=1)\n",
    "X_test = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Delete the dataframes that you do not need anymore to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine training and test dataframes before feature engineering.\n",
    "- **This is not always the correct way.**\n",
    " - For categorial features, this is fine because you want to avoid having new categories in the test set, which will cause different dimensions after dummify the data set.\n",
    " - If you want to perform any transformation (normalization, standardization, etc) on the numerical features, you should **[fit on the training set and transform on the test set.](https://stats.stackexchange.com/a/174865)**\n",
    " - It also applys to how you perform cross-validation. See Chapter 7.10.2 of [ESLR](https://web.stanford.edu/~hastie/Papers/ESLII.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([X_train, X_test], ignore_index=True)\n",
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering - most creative aspect of Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical  features\n",
    "- Nearly always need some treatment\n",
    "- High cardinality can create very sparse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-hot encoding\n",
    "- One-of-K encoding on an array of length K\n",
    "- Basic method: used with most linear algorithm\n",
    "- Drop first column avoids collinearity\n",
    " - encoding gender as two variables, **is_male** and **is_female**, produces two features which are perfectly negatively correlated\n",
    "- Encode categories appearing 3+ times\n",
    " - Reduce training feature space with no loss of info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in all_data.columns:\n",
    "    if all_data[c].dtype == 'object':\n",
    "        print(c, len(all_data[c].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_df = pd.get_dummies(all_data, drop_first=True, dummy_na=True)\n",
    "one_hot_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding\n",
    "- Give every categorial variable a unique numerical ID\n",
    "- Useful for non-linear tree-based algorithm\n",
    "- Does not increase dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_df = all_data.copy()\n",
    "\n",
    "for c in label_df.columns:\n",
    "    if label_df[c].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        # Need to convert the column type to string in order to encode missing values\n",
    "        label_df[c] = le.fit_transform(label_df[c].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Features with many categories - rows:category ratio 20:1 or less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Count encoding\n",
    "- Rank categorical variables by count in the **training** set and transform the test set\n",
    "- Iterate counter for each CV fold - fit on the **new training set** and transform on the **new test set**\n",
    "- Useful for both linear or non-linear algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelCountEncoder(object):\n",
    "    def __init__(self):\n",
    "        self.count_dict = {}\n",
    "    \n",
    "    def fit(self, column):\n",
    "        # This gives you a dictionary with level as the key and counts as the value\n",
    "        count = column.value_counts().to_dict()\n",
    "        # We want to rank the key by its value and use the rank as the new value\n",
    "        # Your code here\n",
    "        # self.count_dict = \n",
    "    \n",
    "    def transform(self, column):\n",
    "        # If a category only appears in the test set, we will assign the value to zero.\n",
    "        missing = 0\n",
    "        # Your code here\n",
    "    \n",
    "    def fit_transform(self, column):\n",
    "        self.fit(column)\n",
    "        return self.transform(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_count_df = X_train.copy()\n",
    "\n",
    "for c in label_count_df.columns:\n",
    "    if label_count_df[c].dtype == 'object':\n",
    "        lce = LabelCountEncoder()\n",
    "        label_count_df[c] = lce.fit_transform(label_count_df[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Features\n",
    "\n",
    "- Label Count encoding is good in general, however, some of the features are ordinal in nature.\n",
    "- For example, we usually consider Excellent > Good > Average/Typical > Fair > Poor\n",
    "- We can construct a dictionary like the following and map it to those columns:\n",
    "  ```python\n",
    "  {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa':2, 'Po':1}\n",
    "  ```\n",
    " \n",
    "- You need a different dictionary for columns with different levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ord_cols = ['ExterQual', 'ExterCond','BsmtCond','HeatingQC', 'KitchenQual', \n",
    "           'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n",
    "ord_dic = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa':2, 'Po':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_df = X_train.copy()\n",
    "\n",
    "for col in ord_cols:\n",
    "    ord_df[col] = ord_df[col].map(lambda x: ord_dic.get(x, 0))\n",
    "ord_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Interactions\n",
    "- If interactions are natural for a problem - ML only does approximations! => sub-optimal\n",
    " - Start from interactions that make sense intuitively. \n",
    " - Winners usually find something that most people struggle to see in data. **Not many people look at the data at all!**\n",
    " \n",
    "|  GarageCond |   GarageType   | GarageCond * GarageType  |\n",
    "| ------------|:--------------:| -----:|\n",
    "|  Ex  | 2Types | Ex * 2Types |\n",
    "|  Ex  | CarPort| Ex * CarPort|\n",
    "|  TA  | Basement| TA * Basement|\n",
    "|  Fa  | BuiltIn | Fa * BuiltIn |\n",
    " \n",
    " \n",
    "- Test your method with all explicitly created possible 2-way interactions if you have enough computing power\n",
    "- This is especially useful when dealing with **anonymous data** (column name unknown)\n",
    "- If 2-way interactions help – go even further (3-way, 4-way, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dealing with NA's depends on situation. NA itself is an information unit! Usually separate category is enough.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features\n",
    "Feature transformations to consider:\n",
    "- Scaling - min/max, N(0,1), root/power scaling, log scaling, Box-Cox, quantiles.\n",
    " - **[Fit on the training set and transform on the test set.](https://stats.stackexchange.com/a/174865)**\n",
    "- Rounding (too much precision might be noise!)\n",
    "- Interactions {+,-,*,/}\n",
    " - Since area related features are very important to determine house prices, we can add one more feature which is the total area of basement, first and second floor areas of each house\n",
    " - `all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']`\n",
    "- **Tree methods are almost invariant to scaling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Everything you need to know about feature engineering is [here](https://www.slideshare.net/HJvanVeen/feature-engineering-72376750?qid=14629b24-6d05-4275-acc9-ea0743605071&v=&b=&from_search=1).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic approach: apply grid search on all parameter space\n",
    "- Zero effort and no supervision\n",
    "- Enormous parameter space\n",
    "- Very time consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expert approach: experience + intuition + resources at hand\n",
    "1. Pick one set of parameters from the Kaggle kernel or the golden parameter you used in the previous competition\n",
    "2. Start with the parameter that doesn't affect the others too much\n",
    " - i.e. learning rate $\\eta $ in boosting method doesn't influence other parameter tuning (from my experience)\n",
    " - `max_depth`, `min_samples_split` and `min_samples_leaf` in random forest are highly correlated with each other\n",
    "3. Iteratively tuning the features that control overfitting/underfitting\n",
    " - If it helps on CV, try to tune it as much as possible. Stop after CV score converges.\n",
    " - You can use public leaderboard as your K+1 fold to further prove it.\n",
    "4. Go back to step 2 and stop when you are satisfied with the result and won't regret not working harder.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Bayesian optimization method](https://github.com/fmfn/BayesianOptimization/blob/master/examples/visualization.ipynb): trade-off between expert and grid search approach\n",
    "- Zero effort and no supervision\n",
    "- Grid space reduced on previous iteration's results (mimic expert decisions)\n",
    "- Time consuming (still)\n",
    "- Easy to integrate with sklearn cross validation function. See [examples](https://github.com/fmfn/BayesianOptimization/blob/master/examples/sklearn_example.py) here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Golden rule: finding optimal configuration rarely is a good time investment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Ensembling by voting](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier)\n",
    "```\n",
    "1111111100 = 80% accuracy \n",
    "0111011101 = 70% accuracy \n",
    "1000101111 = 60% accuracy\n",
    "```\n",
    "**Majority Vote**\n",
    "```\n",
    "1111111101 = 90% accuracy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensembling by averaging\n",
    "- Let’s say we have N predictions from N different models: $y_1, y_2, ... , y_N$\n",
    "- We want to make a single prediction using weighted average: $\\beta_1*y_1+\\beta_2*y_2+...+\\beta_N*y_N$\n",
    "- How do we find the best beta cofficients?\n",
    "- Very common mistake to select weights based on leaderboard feedback\n",
    " - **inefficient & prone to leaderboard overfitting**\n",
    "- Solve the problem using CV predictions with optimization algorithms \n",
    " - $optim(\\beta_1*y_1+\\beta_2*y_2+...+\\beta_N*y_N)$ with starting weights $\\beta_i=1/N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stacked Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure for a 5 fold stacking may be described as follows:\n",
    "\n",
    "1. Split the total training set into two disjoint sets (here train and holdout)\n",
    "\n",
    "2. Train several base models on the first part (train)\n",
    "\n",
    "3. Predict these base models on the second part (holdout)\n",
    "\n",
    "4. Repeat step 1-3 five times and use the holdout predictions as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n",
    "\n",
    "\n",
    "- For the test set, we could either average the predictions of all base models on the test data or refit the model using the whole training set and then predict. Generally speaking, either way is fine because the test set hasn't seen the training set.\n",
    "- If we ran 10 models using the same procedure, our meta model will have 10 input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://s3.amazonaws.com/nycdsabt01/stacking.jpg)\n",
    "\n",
    "Borrowed from [Faron](https://www.kaggle.com/getting-started/18153#post103381)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick note, one should try a few diverse models. To my experience, a good stacking solution is often composed of at least:\n",
    "- 2 or 3 GBMs/XGBs/LightGBMs (one with low depth, one with medium and one with high)\n",
    "- 1 or 2 Random Forests (again as diverse as possible–one low depth, one high)\n",
    "- 1 linear model**!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful if you are debugging the function inside another .py script\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, LinearRegression as lr\n",
    "from sklearn.ensemble import GradientBoostingRegressor as gbr, RandomForestRegressor as rfr\n",
    "from preprocess import impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = impute(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_index = len(X_train)\n",
    "for col in all_data.columns:\n",
    "    if col in ord_cols:\n",
    "        all_data[col] = all_data[col].map(lambda x: ord_dic.get(x, 0))\n",
    "    elif all_data[col].dtype == \"object\":\n",
    "        lce = LabelCountEncoder()\n",
    "        # fit the encoder using just training set\n",
    "        all_data.loc[:train_index, col] = lce.fit_transform(all_data.loc[:train_index, col])\n",
    "        all_data.loc[train_index:, col] = lce.transform(all_data.loc[train_index:, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = all_data.iloc[:train_index, :]\n",
    "X_test = all_data.iloc[train_index:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stacking import stacking_regression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.log(y), np.log(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    # linear model, ElasticNet = lasso + ridge\n",
    "    ElasticNet(random_state=0),\n",
    "    \n",
    "    # conservative random forst model\n",
    "    rfr(random_state=0,\n",
    "        n_estimators=1000, max_depth=6,  max_features='sqrt'),\n",
    "    \n",
    "    # aggressive random forst model\n",
    "    rfr(random_state=0, \n",
    "        n_estimators=1000, max_depth=9,  max_features='auto'),\n",
    "    \n",
    "    # conservative gbm model\n",
    "    gbr(random_state=0, learning_rate = 0.005, max_features='sqrt',\n",
    "        min_samples_leaf=15, min_samples_split=10, \n",
    "        n_estimators=3000, max_depth=3),\n",
    "    \n",
    "    # aggressive gbm model\n",
    "    gbr(random_state = 0, learning_rate = 0.01, max_features='sqrt',\n",
    "        min_samples_leaf=10, min_samples_split=5, \n",
    "        n_estimators = 1000, max_depth = 9)\n",
    "    ]\n",
    "\n",
    "meta_model = lr(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "final_prediction = stacking_regression(models, meta_model, X_train, y_train, X_test,\n",
    "                               transform_target=np.log1p, transform_pred = np.expm1, \n",
    "                               metric=rmsle, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Having more models than necessary in ensemble may hurt.**\n",
    "\n",
    "\n",
    "- Lets say we have a library of created models. Usually greedy-forward approach works well:\n",
    " - Start with a few well-performing models’ ensemble\n",
    " - Loop through each other model in a library and add to current ensemble\n",
    " - Determine best performing ensemble configuration\n",
    " - Repeat until metric converged\n",
    "- If you are using linear regression as the meta model, make sure you have **diverse/uncorrelated** first layer models\n",
    "\n",
    "- During each loop iteration it is wise to consider only a subset of library models, which could work as a regularization for model selection.\n",
    "\n",
    "- Repeating procedure few times and bagging results reduces the possibility of overfitting by doing model selection.\n",
    "\n",
    "- R users can use the `caretStack` function from the [caretEnsemble](https://github.com/zachmayer/caretEnsemble) package directly. A nice tutorial [here](https://machinelearningmastery.com/machine-learning-ensembles-with-r/).\n",
    "\n",
    "- The `stackedEnsemble()` function from the [H2o package](https://h2o-release.s3.amazonaws.com/h2o/rel-ueno/2/docs-website/h2o-docs/data-science/stacked-ensembles.html) is also a good choice out there. But the downside is it only takes h2o model as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Success formula (personal opinion)\n",
    "\n",
    "50% - feature engineering\n",
    "\n",
    "30% - model diversity\n",
    "\n",
    "10% - luck\n",
    "\n",
    "10% - proper ensembling\n",
    " - Voting\n",
    " - Averaging\n",
    " - Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
